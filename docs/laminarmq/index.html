<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Usage"><title>laminarmq - Rust</title><script> if (window.location.protocol !== "file:") document.write(`<link rel="preload" as="font" type="font/woff2" crossorigin href="../static.files/SourceSerif4-Regular-46f98efaafac5295.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../static.files/FiraSans-Regular-018c141bf0843ffd.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../static.files/FiraSans-Medium-8f9a781e4970d388.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../static.files/SourceCodePro-Regular-562dcc5011b6de7d.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../static.files/SourceCodePro-Semibold-d899c5a5c4aeb14a.ttf.woff2">`)</script><link rel="stylesheet" href="../static.files/normalize-76eba96aa4d2e634.css"><link rel="stylesheet" href="../static.files/rustdoc-e935ef01ae1c1829.css"><meta name="rustdoc-vars" data-root-path="../" data-static-root-path="../static.files/" data-current-crate="laminarmq" data-themes="" data-resource-suffix="" data-rustdoc-version="1.78.0 (9b00956e5 2024-04-29)" data-channel="1.78.0" data-search-js="search-42d8da7a6b9792c2.js" data-settings-js="settings-4313503d2e1961c2.js" ><script src="../static.files/storage-4c98445ec4002617.js"></script><script defer src="../crates.js"></script><script defer src="../static.files/main-12cf3b4f4f9dc36d.js"></script><noscript><link rel="stylesheet" href="../static.files/noscript-04d5337699b92874.css"></noscript><link rel="alternate icon" type="image/png" href="../static.files/favicon-16x16-8b506e7a72182f1c.png"><link rel="alternate icon" type="image/png" href="../static.files/favicon-32x32-422f7d1d52889060.png"><link rel="icon" type="image/svg+xml" href="../static.files/favicon-2c020d218678b618.svg"></head><body class="rustdoc mod crate"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle" title="show sidebar"></button></nav><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../laminarmq/index.html">laminarmq</a><span class="version">0.0.5</span></h2></div><div class="sidebar-elems"><ul class="block">
            <li><a id="all-types" href="all.html">All Items</a></li></ul><section><ul class="block"><li><a href="#modules">Modules</a></li></ul></section></div></nav><div class="sidebar-resizer"></div>
    <main><div class="width-limiter"><nav class="sub"><form class="search-form"><span></span><div id="sidebar-button" tabindex="-1"><a href="../laminarmq/all.html" title="show sidebar"></a></div><input class="search-input" name="search" aria-label="Run search in the documentation" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"><div id="help-button" tabindex="-1"><a href="../help.html" title="help">?</a></div><div id="settings-menu" tabindex="-1"><a href="../settings.html" title="settings"><img width="22" height="22" alt="Change settings" src="../static.files/wheel-7b819b6101059cd0.svg"></a></div></form></nav><section id="main-content" class="content"><div class="main-heading"><h1>Crate <a class="mod" href="#">laminarmq</a><button id="copy-path" title="Copy item path to clipboard"><img src="../static.files/clipboard-7571035ce49a181d.svg" width="19" height="18" alt="Copy item path"></button></h1><span class="out-of-band"><a class="src" href="../src/laminarmq/lib.rs.html#1-17">source</a> · <button id="toggle-all-docs" title="collapse all docs">[<span>&#x2212;</span>]</button></span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p align="center">
  <img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/logo.png" alt="laminarmq" />
</p>
<p align="center">
  <a href="https://github.com/arindas/laminarmq/actions/workflows/rust-ci.yml">
  <img src="https://github.com/arindas/laminarmq/actions/workflows/rust-ci.yml/badge.svg" />
  </a>
  <a href="https://codecov.io/gh/arindas/laminarmq" >
  <img src="https://codecov.io/gh/arindas/laminarmq/branch/main/graph/badge.svg?token=6VLETF5REC"/>
  </a>
  <a href="https://crates.io/crates/laminarmq">
  <img src="https://img.shields.io/crates/v/laminarmq" />
  </a>
  <a href="https://github.com/arindas/laminarmq/actions/workflows/rustdoc.yml">
  <img src="https://github.com/arindas/laminarmq/actions/workflows/rustdoc.yml/badge.svg" />
  </a>
</p>
<p align="center">
A scalable, distributed message queue powered by a segmented, partitioned, replicated and immutable
log.<br><i>This is currently a work in progress.</i>
</p>
<h3 id="usage"><a class="doc-anchor" href="#usage">§</a>Usage</h3>
<p><code>laminarmq</code> provides a library crate and two binaries for managing <code>laminarmq</code> deployments. In order
to use <code>laminarmq</code> as a library, add the following to your <code>Cargo.toml</code>:</p>
<div class="example-wrap"><pre class="language-toml"><code>[dependencies]
laminarmq = &quot;0.0.5&quot;
</code></pre></div>
<p>Refer to latest git <a href="https://arindas.github.io/laminarmq/docs/laminarmq/">API Documentation</a> or
<a href="https://docs.rs/laminarmq">Crate Documentation</a> for more details. There’s also a
<a href="https://arindas.github.io/laminarmq/book">book</a> being written to further describe design decisions,
implementation details and recipes.</p>
<p><code>laminarmq</code> presents an elementary commit-log abstraction (a series of records ordered by indices),
on top of which several message queue semantics such as publish subscribe or even full blown
protocols like MQTT could be implemented. Users are free to read the messages with offsets in any
order they need.</p>
<h3 id="major-milestones-for-laminarmq"><a class="doc-anchor" href="#major-milestones-for-laminarmq">§</a>Major milestones for <code>laminarmq</code></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Locally persistent queue of records</li>
<li><input disabled="" type="checkbox"/>
Single node, multi threaded, eBPF based request to thread routed message queue</li>
<li><input disabled="" type="checkbox"/>
Service discovery with
<a href="https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf">SWIM</a>.</li>
<li><input disabled="" type="checkbox"/>
Replication and consensus of replicated records with <a href="https://raft.github.io/raft.pdf">Raft</a>.</li>
</ul>
<h3 id="examples"><a class="doc-anchor" href="#examples">§</a>Examples</h3>
<p>Find examples demonstrating different capabilities of <code>laminarmq</code> in the
<a href="https://github.com/arindas/laminarmq/tree/main/examples">examples</a> directory.</p>
<p>Invoke any example as follows:</p>
<div class="example-wrap"><pre class="language-sh"><code>cargo run --example &lt;example-name&gt; --release
</code></pre></div><h3 id="media"><a class="doc-anchor" href="#media">§</a>Media</h3>
<p>Media associated with the <code>laminarmq</code> project.</p>
<ul>
<li><code>[BLOG]</code> <a href="https://arindas.github.io/blog/segmented-log-rust/">Building Segmented Logs in Rust: From Theory to Production!</a></li>
</ul>
<h3 id="design"><a class="doc-anchor" href="#design">§</a>Design</h3>
<p>This section describes the internal design of <code>laminarmq</code>.</p>
<h4 id="cluster-hierarchy"><a class="doc-anchor" href="#cluster-hierarchy">§</a>Cluster Hierarchy</h4><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-cluster-hierarchy.svg" alt="cluster-hierarchy" />
</p>
<div class="example-wrap"><pre class="language-text"><code>partition_id_x is of the form (topic_id, partition_idx)

In this example, consider:

partition_id_0 = (topic_id_0, partition_idx_0)
partition_id_1 = (topic_id_0, partition_idx_1)

partition_id_2 = (topic_id_1, partition_idx_0)

</code></pre></div>
<blockquote>
<p>The exact numerical ids don’t have any pattern with respect to partition_id and topic_id; there can
be multiple topics, each of which can have multiple partitions (identified by partition_idx).</p>
</blockquote>
<p>… alternatively:</p>
<div class="example-wrap"><pre class="language-text"><code>[cluster]
├── node#001
│   ├── (topic#001, partition#001) [L]
│   │   └── segmented_log{[segment#001, segment#002, ...]}
│   ├── (topic#001, partition#002) [L]
│   │   └── segmented_log{[segment#001, segment#002, ...]}
│   └── (topic#002, partition#001) [F]
│       └── segmented_log{[segment#001, segment#002, ...]}
├── node#002
│   ├── (topic#001, partition#002) [F]
│   │   └── segmented_log{[segment#001, segment#002, ...]}
│   └── (topic#002, partition#001) [L]
│       └── segmented_log{[segment#001, segment#002, ...]}
└── ...other nodes

</code></pre></div><div class="example-wrap"><pre class="language-text"><code>[L] := leader; [F] := follower
</code></pre></div><p align="center">
<b>Fig:</b> <code>laminarmq</code> cluster hierarchy depicting partitioning and replication.
</p>
<p>A “topic” is a collection of records. A topic is divided into multiple “partition”(s). Each
“partition” is then further replicated across multiple “node”(s). A “node” may contain some or all
“partition”(s) of a “topic”. In this way a topic is both partitioned and replicated across the
nodes in the cluster.</p>
<p>There is no ordering of messages at the “topic” level. However, a “partition” is an ordered
collection of records, ordered by record indices.</p>
<p>Although we conceptually maintain a hierarchy of partitions and topics, at the cluster level, we
have chosen to maintain a flat representation of topic partitions. We present an elementary
commit-log API at the partition level.</p>
<p>Users may hence do the following:</p>
<ul>
<li>Directly interact with our message queue at the partition level</li>
<li>Use client side load balancing between topic partitions</li>
</ul>
<p>This alleviates the burden of load balancing messages among partitions and message stream ownership
record keeping from the cluster. Higher level constructs can be built on top of the partition
commit-log based API as necessary.</p>
<p>Each partition replica group has a leader where writes go, and a set of followers which follow the
leader and may be read from. Users may again use client side load balancing to balance reads across
the leader and all the followers.</p>
<p>Each partition replica is backed by a segmented log for storage.</p>
<h4 id="service-discovery-and-partition-distribution-to-nodes"><a class="doc-anchor" href="#service-discovery-and-partition-distribution-to-nodes">§</a>Service discovery and partition distribution to nodes</h4><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-service-discovery-and-partition-distribution.svg" alt="service-discovery-and-partition-distribution-to-nodes"/>
</p>
<p align="center">
<b>Fig:</b> Rendezvous hashing based partition distribution and gossip style service discovery
mechanism used by <code>laminarmq</code>
</p>
<p>In our cluster, nodes discover each other using gossip style peer to peer mechanisms. One such
mechanism is <a href="https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf">SWIM</a> (Scalable
Weakly Consistent Infection-style Process Group Memberhsip).</p>
<p>In this mechanism, each member node notifies other members in the group whether a node is joining or
leaving the cluster by using a gossip style information dissemination mechanism (A node gossips to
neighbouring nodes, they in-turn gossip to their neighbours and so on).</p>
<p>In order to see whether a node has failed, the nodes randomly probes individual nodes in the
cluster. For instance, node A probes node B directly. If node B responds, it has not failed. If node
B does not respond, A attempts to probe node B indirectly through other nodes in the cluster, e.g.
node A might ask node C to probe node B. Node A continues to indirectly probe node B with all the
other nodes in the cluster. If node B responds to any of the indirect probes, it is still considered
to not have failed. It is otherwise declared failed and removed from the cluster.</p>
<p>There are mechanisms in place to reduce false failures caused due to temporary hiccups. The paper
goes into detail about those mechanisms.</p>
<p>This is also the core technology used in <a href="https://www.serf.io/">Hashicorp Serf</a>, where there are
further enhancements to improve failure detection and convergence speed.</p>
<p>Using this mechanism we can obtain a list of all the members in our cluster, along with their unique
ids and capacity weights. We then use their ids and weights to determine where to place a partition
using Rendezvous hashing.</p>
<p>From the Wikipedia <a href="https://en.wikipedia.org/wiki/Rendezvous_hashing">article</a>:</p>
<blockquote>
<p>Rendezvous or highest random weight (HRW) hashing is an algorithm that allows clients to achieve
distributed agreement on a set of <em>k</em> options out of a possible set of <em>n</em> options. A typical
application is when clients need to agree on which sites (or proxies) objects are assigned to.</p>
</blockquote>
<p>In our case, we use rendezvous hashing to determine the subset of nodes to use for placing the
replicas of a partition.</p>
<p>For some hashing function <code>H</code>, some weight function <code>f(w, hash)</code> and partition id <code>P_x</code>, we proceed
as follows:</p>
<ul>
<li>For every node <code>N_i</code> in the cluster with a weight <code>w_i</code>, we compute <code>R_i = f(w_i, H(concat(P_x, N_i)))</code></li>
<li>We rank all nodes <code>N_i</code> belonging to the set of nodes <code>N</code> with respect to their rank value <code>R_i</code>.</li>
<li>For some replication factor <code>k</code>, we select the top <code>k</code> nodes to place the <code>k</code> replicas of the
partition with id <code>P_x</code></li>
</ul>
<p>(We assume <code>k &lt;= |N|</code>; where <code>|N|</code> is the number of nodes and <code>k</code> is the number of replicas)</p>
<p>With this mechanism, anyone with the ids and weights of all the members in the cluster can compute
the destination nodes for the replicas of a partition. This knowledge can also be used to route
partition request to the appropriate nodes at both the client side and the server side.</p>
<p>In our case, we use client side load balancing to load balance all idempotent partition requests
across all the possible nodes where a replica of the request’s partition can be present. For
non-idempotent request, if we send it to any one of the candidate nodes, they redirect it to the
current leader of the replica set.</p>
<h4 id="supported-execution-models"><a class="doc-anchor" href="#supported-execution-models">§</a>Supported execution models</h4>
<p><code>laminarmq</code> supports two execution models:</p>
<ul>
<li>General async execution model used by various async runtimes in the Rust ecosystem (e.g <code>tokio</code>)</li>
<li>Thread per core execution model</li>
</ul>
<p>In the thread-per-core execution model individual processor cores are limited to single threads.
This model encourages design that minimizes inter-thread contention and locks, thereby improving
tail latencies in software services. Read: <a href="https://helda.helsinki.fi//bitstream/handle/10138/313642/tpc_ancs19.pdf?sequence=1">The Impact of Thread per Core Architecture on
Application Tail Latency.</a></p>
<p>In the thread per core execution model, we have to leverage application level partitioning such that
each individual thread is responsible for a subset of requests and/or responsibilities. We also have
to complement this model with proper routing of requests to the threads to ensure locality of
requests. In our case this translates to having each thread be responsible for only a subset of the
partition replicas in a node. Requests pertaining to a partition replica are always routed to the
same thread. The following sections will go into more detail as to how this is achieved.</p>
<p>We realize that although the thread per core execution model has some inherent advantages, being
compatible with the existing Rust ecosystem will significantly increase adoption. Therefore, we have
designed our system with reusable components which can be organized to suit both execution models.</p>
<h4 id="request-routing-in-nodes"><a class="doc-anchor" href="#request-routing-in-nodes">§</a>Request routing in nodes</h4><h5 id="general-design"><a class="doc-anchor" href="#general-design">§</a>General design</h5><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-node-request-routing-general.svg" alt="request-rouing-general"/>
</p>
<p align="center">
<b>Fig:</b> Request routing mechanism in <code>laminarmq</code> nodes using the general execution
model.
</p>
<p>In our cluster, we have two kinds of requests:</p>
<ul>
<li><strong>membership requests</strong>: used by the gossip style service discovery system for maintaining cluster
membership.</li>
<li><strong>partition requests</strong>: used to interact with <code>laminarmq</code> topic partitions.</li>
</ul>
<p>We use an <a href="https://ebpf.io/what-is-ebpf/">eBPF</a> XDP filter to classify request packets at the socket
layer into membership request packets and partition request packets. Next we use eBPF to route
membership packets to a different socket which is exclusively used by the membership management
subsystem in that node. The partition request packets are left to flow as is.</p>
<p>Next we have an “HTTP server”, which parses the incoming partition request packets from the original
socket into valid <code>partition::*</code> requests. For every <code>partition::*</code> request, the HTTP server spawns
a future to handle it. This request handler future does the following:</p>
<ul>
<li>Create a new channel <code>(tx, rx)</code> for the request.</li>
<li>Send the parsed partition request along with send end of the channel <code>(partition::*, tx)</code> to the
“Request Router” over the request router’s receiving channel.</li>
<li>Await on the recv. end of the channel created by this future for the response. <code>res = rx.await</code></li>
<li>When we receive the response from this future’s channel, we serialize it and respond back to the
socket we had received the packets from.</li>
</ul>
<p>Next we have a “Request Router / Partition manager” responsible for routing various requests to the
partition serving futures. The request router unit receives both <code>membership::*</code> requests from the
membership subsystem and <code>partition::*</code> requests received from the “HTTP server” request handler
futures (also called request poller futures from here on since they poll for the response from the
channel recv. <code>rx</code> end). The request router unit routes requests as follows:</p>
<ul>
<li><code>membership::*</code> requests are broadcast to all the partition serving futures</li>
<li><code>(partition::*_request(partition_id_x, …), tx)</code> tuples are routed to their destination partitions
using the <code>partition_id</code>.</li>
<li><code>(partition::create(partition_id_x, …), tx)</code> tuples are handled by the request router/ partition
manager itself. For this, the request router / partition manager creates a new partition serving
future, allocates the required storage units or it and sends and appropriate response on <code>tx</code>.</li>
</ul>
<p>Finally, the individual partition server futures receive both <code>membership::*</code> and <code>(partition::*, tx)</code> requests as they come to our node and routed. They handle the requests as necessary and send a
response back to <code>tx</code> where applicable.</p>
<h5 id="thread-per-core-execution-model-compatible-design"><a class="doc-anchor" href="#thread-per-core-execution-model-compatible-design">§</a>Thread per core execution model compatible design</h5><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-node-request-routing-thread-per-core.svg" alt="request-routing-thread-per-core" />
</p>
<p align="center">
<b>Fig:</b> Request routing mechanism in <code>laminarmq</code> nodes using the thred per core
execution model.
</p>
<p>In the thread per core execution model each thread is responsible for a subset of the partitions.
Hence each thread has it’s own “Request Router / Partition Manager”, “HTTP Server” and a set of
partition serving futures. We run multiple such threads on different processor cores.</p>
<p>Now, as discussed before we need to route individual requests to the correct destination thread to
ensure request locality. We use a dedicated “Thread Router” eBPF XDP filter to route partition
request packets to their destination threads.</p>
<p>The “Thread Router” eBPF XDP filter keeps a eBPF <code>sockmap</code> which contains the sockets where each of
the threads listen to for requests. For every incoming request, we route it to its destination
thread using this <code>sockmap</code>. Now we can again leverage rendezvous hashing here to determine the
thread to be used for a request. We use the <code>partition_id</code> and <code>thread_id</code> for rendezvous hashing.
Since all the threads run on different processor cores, they will have similar request handling
capacity and hence will have equal weights. Using this, requests belonging to a particular partition
will always be routed to the same thread on a particular node. This ensures a high level of request
locality.</p>
<p>The remaining components behave as discussed above. Notice how we are able to reuse the same
components in a drastically different execution model, as promised before.</p>
<h4 id="partition-control-flow-and-replication"><a class="doc-anchor" href="#partition-control-flow-and-replication">§</a>Partition control flow and replication</h4><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-partition-control-flow-and-replication.svg" alt="partition-control-flow-replication" />
</p>
<p align="center">
<b>Fig:</b> Partition serving future control flow and partition replication mechanism in
<code>laminarmq</code>
</p>
<p>The partition controller future receives membership event requests <code>membership::{join, leave, update_weight}</code> or <code>(paritition::*, tx)</code> requests from the request router future.</p>
<p>The partition request handler handles the different requests as follows:</p>
<ul>
<li>
<p>Idempotent <code>partition::*_request</code>: performs the necessary idempotent operation on the underlying
segmented log and responds with the result on the response channel.</p>
</li>
<li>
<p>Non-idempotent <code>partition::*_request</code>: Leader and follower replicas handle non-idempotent replicas
differently:</p>
<ul>
<li>Leader replicas: Replicates non-idempotent operations on all follower partitions in the Raft
group if this partition is a leader, and then applies the operation locally. This might involve
first sending an Raft append request, locally writing once majority of replicas respond back,
then commit-ing it locally and finally relay the commit to all other replicas. Leader replicas
respond to requests only from clients. Non-idempotent requests from follower replicas are
ignored.</li>
<li>Follower replicas: Follower replicas respond to non-idempotent requests only from leaders.
Non-idempotent from clients are redirected to the leader. A follower replica handles
non-idempotent requests by applying the changes locally in accordance with Raft.</li>
</ul>
<p>Once the replicas are done handling the request, they send back an appropriate response to the
response channel, if present. (A redirect response is also encoded properly and sent back to the
response channel).</p>
</li>
<li>
<p><code>membership::join(i)</code>: add {node #i} to local priority queue. If the required number of replicas
is more than the current number, pop() one member from the priority queue and add it to the Raft
group (making it an eligible candidate in the Raft leader election process). If the current
replica is a leader, we send a <code>partition::create(...)</code> request. If there is no leader among the
replicas, we initial the leadership election process with each replica as a candidate.</p>
</li>
<li>
<p><code>membership::leave(j)</code>: remove {node #j} from priority queue and Raft group if present. If <code>{node #j}</code> was not present in the Raft group no further action is necessary. If it was present in the
Raft group, <code>pop()</code> another member from the priority queue, add it to the Raft group and proceed
similarly as in the case of <code>membership::join(j)</code></p>
</li>
<li>
<p><code>membership::update_weight(k)</code>: updates priority for <code>{node #k}</code> by recomputing rendezvous_hash
for {node #k} with this partition replicas partition_id. Next, if any node in the priority queue
has a higher priority than any of the nodes in the Raft group, the node with the least priority
is replaced by the highest priority element from the queue. We send a
<code>partition::remove(partition_id, ...)</code> request to <code>{node #k}</code>. Afterwards we proceed similarly
to <code>membership::{leave, join}</code> requests.</p>
</li>
</ul>
<p>When a node goes down the appropriate <code>membership::leave(i)</code> message (where <code>i</code> is the node that
went down) is sent to all the nodes in the cluster. The partition replica controllers in each node
handle the membership request accordingly. In effect:</p>
<ul>
<li>For every leader partition in that node:
<ul>
<li>if there are no other follower replicas in other nodes in it’s Raft group, that partition goes
down.</li>
<li>if there are other follower replicas in other nodes, there are leader elections among them and
after a leader is elected, reads and writes for that partition proceed normally</li>
</ul>
</li>
<li>For every follower partition in that node:
<ul>
<li>the remaining replicas in the same raft group continue to function in accordance with Raft’s
mechanisms.</li>
</ul>
</li>
</ul>
<p>For each of the partition replicas on the node that went down, new host nodes are selected using
rendezvous hash priority.</p>
<p>In our system, we use different Raft groups for different data buckets (replica groups).
<a href="https://www.cockroachlabs.com/">CockroachDB</a> and <a href="https://tikv.org">Tikv</a> call this manner of using
different Raft groups for different data buckets on the same node as MultiRaft.</p>
<p>Read more here:</p>
<ul>
<li><a href="https://tikv.org/deep-dive/scalability/multi-raft/">https://tikv.org/deep-dive/scalability/multi-raft/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/scaling-raft/">https://www.cockroachlabs.com/blog/scaling-raft/</a></li>
</ul>
<p>Every partition controller is backed by a <code>segmented_log</code> for persisting records.</p>
<h4 id="persistence-mechanism"><a class="doc-anchor" href="#persistence-mechanism">§</a>Persistence mechanism</h4><h5 id="segmented_log-persistent-data-structure-for-storing-records-in-a-partition"><a class="doc-anchor" href="#segmented_log-persistent-data-structure-for-storing-records-in-a-partition">§</a><code>segmented_log</code>: Persistent data structure for storing records in a partition</h5>
<p>The segmented-log data structure for storing was originally described in the <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf">Apache
Kafka</a> paper.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-segmented-log.svg" alt="segmented_log"/>
</p>
<p align="center">
<b>Fig:</b> Data organisation for persisting the <code>segmented_log</code> data structure on a
<code>*nix</code> file system.
</p>
<p>A segmented log is a collection of read segments and a single write segment. Each “segment” is
backed by a storage file on disk called “store”.</p>
<p>The log is:</p>
<ul>
<li>“immutable”, since only “append”, “read” and “truncate” operations are allowed. It is not possible
to update or delete records from the middle of the log.</li>
<li>“segmented”, since it is composed of segments, where each segment services records from a
particular range of offsets.</li>
</ul>
<p>All writes go to the write segment. A new record is written at <code>offset = write_segment.next_offset</code>
in the write segment. When we max out the capacity of the write segment, we close the write segment
and reopen it as a read segment. The re-opened segment is added to the list of read segments. A new
write segment is then created with <code>base_offset</code> equal to the <code>next_offset</code> of the previous write
segment.</p>
<p>When reading from a particular offset, we linearly check which segment contains the given read
segment. If a segment capable of servicing a read from the given offset is found, we read from that
segment. If no such segment is found among the read segments, we default to the write segment. The
following scenarios may occur when reading from the write segment in this case:</p>
<ul>
<li>The write segment has synced the messages including the message at the given offset. In this case
the record is read successfully and returned.</li>
<li>The write segment hasn’t synced the data at the given offset. In this case the read fails with a
segment I/O error.</li>
<li>If the offset is out of bounds of even the write segment, we return an “out of bounds” error.</li>
</ul>
<h5 id="laminarmq-specific-enhancements-to-the-segmented_log-data-structure"><a class="doc-anchor" href="#laminarmq-specific-enhancements-to-the-segmented_log-data-structure">§</a><code>laminarmq</code> specific enhancements to the <code>segmented_log</code> data structure</h5>
<p>While the conventional <code>segmented_log</code> data structure is quite performant for a <code>commit_log</code>
implementation, it still requires the following properties to hold true for the record being
appended:</p>
<ul>
<li>We have the entire record in memory</li>
<li>We know the record bytes’ length and record bytes’ checksum before the record is appended</li>
</ul>
<p>It’s not possible to know this information when the record bytes are read from an asynchronous
stream of bytes. Without the enhancements, we would have to concatenate intermediate byte buffers to
a vector. This would not only incur more allocations, but also slow down our system.</p>
<p>Hence, to accommodate this use case, we introduced an intermediate indexing layer to our design.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-indexed-segmented-log-landscape.svg" alt="segmented_log" />
</p>
<div class="example-wrap"><pre class="language-text"><code>//! Index and position invariants across segmented_log

// segmented_log index invariants
segmented_log.lowest_index  = segmented_log.read_segments[0].lowest_index
segmented_log.highest_index = segmented_log.write_segment.highest_index

// record position invariants in store
records[i+1].position = records[i].position + records[i].record_header.length

// segment index invariants in segmented_log
segments[i+1].base_index = segments[i].highest_index = segments[i].index[index.len-1].index + 1
</code></pre></div><p align="center">
<b>Fig:</b> Data organisation for persisting the <code>segmented_log</code> data structure on a
<code>*nix</code> file system.
</p>
<p>In the new design, instead of referring to records with a raw offset, we refer to them with indices.
The index in each segment translates the record indices to raw file position in the segment store
file.</p>
<p>Now, the store append operation accepts an asynchronous stream of bytes instead of a contiguously
laid out slice of bytes. We use this operation to write the record bytes, and at the time of writing
the record bytes, we calculate the record bytes’ length and checksum. Once we are done writing the
record bytes to the store, we write it’s corresponding <code>record_header</code> (containing the checksum and
length), position and index as an <code>index_record</code> in the segment index.</p>
<p>This provides two quality of life enhancements:</p>
<ul>
<li>Allow asynchronous streaming writes, without having to concatenate intermediate byte buffers</li>
<li>Records are accessed much more easily with easy to use indices</li>
</ul>
<p>Now, to prevent a malicious user from overloading our storage capacity and memory with a maliciously
crafted request which infinitely loops over some data and sends it to our server, we have provided
an optional <code>append_threshold</code> parameter to all append operations. When provided, it prevents
streaming append writes to write more bytes than the provided <code>append_threshold</code>.</p>
<p>At the segment level, this requires us to keep a segment overflow capacity. All segment append
operations now use <code>segment_capacity - segment.size + segment_overflow_capacity</code> as the
<code>append_threshold</code> value. A good <code>segment_overflow_capacity</code> value could be <code>segment_capacity / 2</code>.</p>
<h4 id="execution-model"><a class="doc-anchor" href="#execution-model">§</a>Execution Model</h4><h5 id="general-async-runtime-eg-tokio-async-std-etc"><a class="doc-anchor" href="#general-async-runtime-eg-tokio-async-std-etc">§</a>General async runtime (e.g. <code>tokio</code>, <code>async-std</code> etc.)</h5><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-async-execution-model-general.svg" alt="async-execution-model-general" />
</p>
<p align="center">
<b>Fig:</b> General async runtime based execution model for <code>laminarmq</code>
</p>
<p>This execution model is based on the executor, reactor, waker abstractions used by all rust async
runtimes. We don’t have to specifically care about how and where a particular future is executed.</p>
<p>The data flow in this execution model is as follows:</p>
<ul>
<li>A HTTP server future parses HTTP requests from the request socket</li>
<li>For every HTTP request it creates a new future to handle it</li>
<li>The HTTP handler future sends the request and a response channel tx to the request router via a channel.
It also awaits on the response rx end.</li>
<li>The request router future maintains a map of partition_id to designated request channel tx for each
partition controller future.</li>
<li>For every partition request received it forwards the request on the appropriate partition request
channel tx. If a <code>partition::create(...)</code> request is received it creates a new partition controller
future.</li>
<li>The partition controller future send back the response to the provided response channel tx.</li>
<li>The response poller future received it and responds back with a serialized response to the socket.</li>
</ul>
<p>All futures are spawned using the async runtime’s designated <code>{…}::spawn(…)</code> method. We don’t have
to specify any details as to how and where the future’s corresponding task will be executed.</p>
<h5 id="thread-per-core-async-runtime-eg-glommio"><a class="doc-anchor" href="#thread-per-core-async-runtime-eg-glommio">§</a>Thread per core async runtime (e.g. <code>glommio</code>)</h5><p align="center">
<img src="https://raw.githubusercontent.com/arindas/laminarmq/assets/assets/diagrams/laminarmq-async-execution-model-thread-per-core.svg" alt="async-execution-model-thread-per-core"/>
</p>
<p align="center">
<b>Fig:</b> Thread per core async runtime based execution model for <code>laminarmq</code>
</p>
<p>In the thread per core model since each processor core is limited to a single thread, tasks in a
thread need to be scheduled efficiently. Hence each worker thread runs their own task scheduler.</p>
<p>We currently use <a href="https://docs.rs/glommio"><code>glommio</code></a> as our thread-per-core runtime.</p>
<p>Here, tasks can be scheduled on different task queues and different task queues can be provisioned
with specific fractions of CPU time shares. Generally tasks with similar latency profiles are
executed on the same task queue. For instance web server tasks will be executed on a different queue
than the one that runs tasks for persisting data to the disk.</p>
<p>We re-use the same constructs that we use in the general async runtime execution model. The only
difference being, we explicitly care about in which task queue a class of future’s tasks are
executed. In our case, we have the following 4 task queues:</p>
<ul>
<li>Request router task queue</li>
<li>HTTP server request parser task queue</li>
<li>Partition replica controller task queue</li>
<li>Response poller task queue</li>
</ul>
<p>Each of these task queue can be assigned specific fractions of CPU time shares. <code>glommio</code> also
provides utilities for automatically deducing these CPU time shares based on their runtime latency
profiles.</p>
<p>Apart from this <code>glommio</code> leverages the new linux 5.x <a href="https://kernel.dk/io_uring.pdf"><code>io_uring</code></a>
API which facilitates true asynchronous IO for both networking and disk interfaces. (Other <code>async</code>
runtimes such as <a href="https://docs.rs/tokio"><code>tokio</code></a> make blocking system calls for disk IO operations
in a thread-pool.)</p>
<p><code>io_uring</code> also has the advantage of being able to queue together multiple system calls together and
then asynchronously wait for their completion by making a maximum of one context switch. It is also
possible to avoid context switches altogether. This is achieved with a pair of ring buffers called
the submission-queue and the completion-queue. Once the queues are set up, user can queue multiple
system calls on the submission queue. The linux kernel processes the system calls and places the
results in the completion queue. The user can then freely read the results from the
completion-queue. This entire process after setting up the queues doesn’t require any additional
context switch.</p>
<p>Read more: <a href="https://man.archlinux.org/man/io_uring.7.en">https://man.archlinux.org/man/io_uring.7.en</a></p>
<p><code>glommio</code> presents additional abstractions on top of <code>io_uring</code> in the form of an async runtime,
with support for networking, disk IO, channels, single threaded locks and more.</p>
<p>Read more: <a href="https://www.datadoghq.com/blog/engineering/introducing-glommio/">https://www.datadoghq.com/blog/engineering/introducing-glommio/</a></p>
<h3 id="testing"><a class="doc-anchor" href="#testing">§</a>Testing</h3>
<p>You may run tests with <code>cargo</code> as you would for any other crate. However, since <code>laminarmq</code> is
poised to support multiple runtimes, some of them might require some additional setup before running
the steps.</p>
<p>For instance, the <code>glommio</code> async runtime which requires an updated linux kernel (at least 5.8) with
<code>io_uring</code> support. <code>glommio</code> also requires at least 512 KiB of locked memory for <code>io_uring</code> to
work. (Note: 512 KiB is the minimum needed to spawn a single executor. Spawning multiple executors
may require you to raise the limit accordingly. I recommend 8192 KiB on a 8 GiB RAM machine.)</p>
<p>First, check the current <code>memlock</code> limit:</p>
<div class="example-wrap"><pre class="language-sh"><code>ulimit -l

# 512 ## sample output
</code></pre></div>
<p>If the <code>memlock</code> resource limit (rlimit) is lesser than 512 KiB, you can increase it as follows:</p>
<div class="example-wrap"><pre class="language-sh"><code>sudo vi /etc/security/limits.conf
*    hard    memlock        512
*    soft    memlock        512
</code></pre></div>
<p>To make the new limits effective, you need to log in to the machine again. Verify whether the limits
have been reflected with <code>ulimit</code> as described above.</p>
<blockquote>
<p>(On old WSL versions, you might need to spawn a login shell every time for the limits to be
reflected:</p>
<div class="example-wrap"><pre class="language-sh"><code>su ${USER} -l
</code></pre></div>
<p>The limits persist once inside the login shell. This is not necessary on the latest WSL2 version as
of 22.12.2022)</p>
</blockquote>
<p>Finally, clone the repository and run the tests:</p>
<div class="example-wrap"><pre class="language-sh"><code>git clone https://github.com/arindas/laminarmq.git
cd laminarmq/
cargo test
</code></pre></div><h3 id="benchmarking"><a class="doc-anchor" href="#benchmarking">§</a>Benchmarking</h3>
<p>Same pre-requisites as testing. Once the pre-requisites are satisfied you may
run benchmarks with <code>cargo</code> as usual:</p>
<div class="example-wrap"><pre class="language-sh"><code>git clone https://github.com/arindas/laminarmq.git
cd laminarmq/
cargo bench
</code></pre></div>
<p>The complete latest benchmark reports are available at <a href="https://arindas.github.io/laminarmq/bench/latest/report">https://arindas.github.io/laminarmq/bench/latest/report</a>.</p>
<p>All benchmarks in the reports have been run on a machine (HP Pavilion x360 Convertible 14-ba0xx) with:</p>
<ul>
<li>4 core CPU (Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz)</li>
<li>8GB RAM (SK Hynix HMA81GS6AFR8N-UH DDR4 2133 MT/s)</li>
<li>128GB SSD storage (SanDisk SD8SN8U-128G-1006)</li>
</ul>
<h4 id="selected-benchmark-reports"><a class="doc-anchor" href="#selected-benchmark-reports">§</a>Selected Benchmark Reports</h4>
<p>This section presents some selected benchmark reports.</p>
<blockquote>
<p><strong>Note</strong>: We use the following names for different record sizes:</p>
<table>
   <tr>
       <td><b>size_name</b></td>
       <td><b>size</b></td>
       <td><b>comments</b></td>
   </tr>
   <tr>
       <td><code>tiny</code></td>
       <td><code>12 bytes</code></td>
       <td>none</td>
   </tr>
   <tr>
       <td><code>tweet</code></td>
       <td><code>140 bytes</code></td>
       <td>none</td>
   </tr>
   <tr>
       <td><code>half_k</code></td>
       <td><code>560 bytes</code></td>
       <td><code>≈ 512 bytes</code></td>
   </tr>
   <tr>
       <td><code>k</code></td>
       <td><code>1120 bytes</code></td>
       <td><code>≈ 1024 bytes (1 KiB)</code></td>
   </tr>
   <tr>
       <td><code>linked_in_post</code></td>
       <td><code>2940 bytes</code></td>
       <td><code>≤ 3000 bytes (3 KB)</code></td>
   </tr>
   <tr>
       <td><code>blog</code></td>
       <td><code>11760 bytes (11.76 KB)</code></td>
       <td><code>4X linked_in_post</code></td>
   </tr>
</table>
</blockquote>
<h5 id="commit_log-write-benchmark-with-1kb-messages"><a class="doc-anchor" href="#commit_log-write-benchmark-with-1kb-messages">§</a><code>commit_log</code> write benchmark with 1KB messages</h5><p align="center">
<img src="https://svg-add-bg-fn.vercel.app/?svg=https://arindas.github.io/laminarmq/bench/latest/commit_log_append_with_k_message/report/lines.svg" alt="k-message-write-bench"/>
</p>
<p align="center">
<b>Fig:</b> Comparing Time taken v/s Input size in bytes (lower is better) across storage back-ends
</p>
<p>View this benchmark report in more detail <a href="https://arindas.github.io/laminarmq/bench/latest/commit_log_append_with_k_message/report/index.html">here</a></p>
<p>This benchmark measures the time taken to write messages of size 1KB across different <code>commit_log</code> storage back-ends.</p>
<p>We also profile our implementation across different storage backends. Here’s a
profile using the
<a href="https://arindas.github.io/laminarmq/docs/laminarmq/storage/impls/glommio/storage/dma/struct.DmaStorage.html"><code>DmaStorage</code></a>
backend.</p>
<p align="center">
<a href="https://arindas.github.io/laminarmq/bench/latest/commit_log_append_with_k_message/glommio_dma_file_segmented_log/10000/profile/flamegraph.svg">
<img src="https://arindas.github.io/laminarmq/bench/latest/commit_log_append_with_k_message/glommio_dma_file_segmented_log/10000/profile/flamegraph.svg" alt="flamegraph">
</a>
</p>
<p align="center">
<b>Fig:</b> Flamegraph for 10,000 writes of 1KB messages on DmaStorage backend
</p>
<p>As you can see, a lot of time is spent simply hashing the request bytes.</p>
<h5 id="segmented_log-streaming-read-benchmark-with-1kb-messages"><a class="doc-anchor" href="#segmented_log-streaming-read-benchmark-with-1kb-messages">§</a><code>segmented_log</code> streaming read benchmark with 1KB messages</h5><p align="center">
<img src="https://svg-add-bg-fn.vercel.app/?svg=https://arindas.github.io/laminarmq/bench/latest/segmented_log_read_stream_with_k_message/report/lines.svg" alt="k-message-read-bench"/>
</p>
<p align="center">
<b>Fig:</b> Comparing Time taken v/s Input size in bytes (lower is better) across storage back-ends
</p>
<p>View this benchmark report in more detail <a href="https://arindas.github.io/laminarmq/bench/latest/segmented_log_read_stream_with_k_message/report/index.html">here</a></p>
<p>This benchmark measures the time taken for streaming reads on messages of size
1KB across different <code>segmented_log</code> storage back-ends.</p>
<p>We also profile our implementation across different storage backends. Here’s a
profile using the
<a href="https://arindas.github.io/laminarmq/docs/laminarmq/storage/impls/glommio/storage/dma/struct.DmaStorage.html"><code>DmaStorage</code></a>
backend.</p>
<p align="center">
<a href="https://arindas.github.io/laminarmq/bench/latest/segmented_log_read_stream_with_k_message/glommio_dma_file_segmented_log/10000/profile/flamegraph.svg">
<img src="https://arindas.github.io/laminarmq/bench/latest/segmented_log_read_stream_with_k_message/glommio_dma_file_segmented_log/10000/profile/flamegraph.svg" alt="flamegraph">
</a>
</p>
<p align="center">
<b>Fig:</b> Flamegraph for 10,000 reads of 1KB messages on DmaStorage backend
</p>
<p>In this case, more time is spent on system calls and I/O.</p>
<p>The remaining benchmark reports are available at <a href="https://arindas.github.io/laminarmq/bench/latest/report">https://arindas.github.io/laminarmq/bench/latest/report</a>.</p>
<h3 id="license"><a class="doc-anchor" href="#license">§</a>License</h3>
<p><code>laminarmq</code> is licensed under the MIT License. See
<a href="https://raw.githubusercontent.com/arindas/laminarmq/main/LICENSE">License</a> for more details.</p>
</div></details><h2 id="modules" class="section-header">Modules<a href="#modules" class="anchor">§</a></h2><ul class="item-table"><li><div class="item-name"><a class="mod" href="common/index.html" title="mod laminarmq::common">common</a></div><div class="desc docblock-short">Module containing common utilities used throughout <code>laminarmq</code>.</div></li><li><div class="item-name"><a class="mod" href="prelude/index.html" title="mod laminarmq::prelude">prelude</a></div><div class="desc docblock-short">Prelude module for <a href="index.html" title="mod laminarmq"><code>laminarmq</code></a> with common exports for convenience.</div></li><li><div class="item-name"><a class="mod" href="server/index.html" title="mod laminarmq::server">server</a></div><div class="desc docblock-short">Module providing abstractions for commit-log based message queue RPC server.</div></li><li><div class="item-name"><a class="mod" href="storage/index.html" title="mod laminarmq::storage">storage</a></div><div class="desc docblock-short">Module providing abstractions to store records.</div></li></ul></section></div></main></body></html>